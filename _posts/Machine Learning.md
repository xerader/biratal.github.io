---
title: My Page
date created: Tuesday, October 29th 2024, 10:36:20 am
date modified: Wednesday, January 8th 2025, 5:10:22 pm
---

%% Begin Waypoint %%
- [[Activation functions]]
- [[Attention]]
- [[Backpropagation]]
- [[Bayesian Method]]
- [[Bias-Variance Tradeoff]]
- **[[Books]]**

- [[Bounds, and revision of probability]]
- [[Clustering]]
- [[Confounding]]
- [[Convolutional Neural Networks]]
- [[Cost Function]]
- **[[Deep Learning]]**
	- [[To understand GluNet]]
- [[Dimensionality Reduction]]
- [[Empirical Risk Minimization]]
- [[Exercises on Backpropagation]]
- [[Gradient Descent]]
- [[Important Terms]]
- [[Information Theory]]
- [[Layers]]
- [[Linear Regression in R]]
- [[Logistic Regression Hypothesis and Frequentist method]]
- [[Neural Networks]]
- **[[NLP]]**
- [[RAGs]]
- [[Softmax]]
- [[Stochastic Gradient Descent]]
- [[Supervised Learning]]
- **[[Talks]]**
	- [[Autonomous Drone Systems]]
	- [[GenAI in Teaching]]
- [[Training and Test Error]]
- [[Transformer]]
- [[Untitled]]
- [[Validation]]

%% End Waypoint %%


--- 

![[Machine Learning 2024-12-19 21.39.21|center|450]]

# History of ML

## Best Fit line
It started with Gauss. He wanted to best fit a line through data. 
## The Neural Network
McCullock and Pitts wrote down the calculus that was "Neuronal activated" using boolean algebra. The idea of a neuron as a computational element. [[Other Fields/Machine Learning/Neural Networks]]. 
## 1956 AI Conference at Dartmouth
John McCarthy proposed the term "Artificial Intelligence" at the 10 week summer project at Dartmouth College. 
## 1958 Frank Rosenblatt's Perceptron
Established the idea of a Perceptron. MLP pays homage to him through the naming. 
## 1980s Yann LeCun's - Convolutional Neural Network
Built the convolutional neural network. Used to determine the zip codes from an image of text. UNET model. 
## 1995 Support Vector Machine, Vapnik
Maximum multi classification. It classifies stuff in a way that is the "most" clear. 
## 2012 AlexNet and Deep Learning, Hinton
This is a multilayer neural network. He got it to work with NVIDIA (GPU to train). Used ReLU as an [[Activation functions|activation]] function instead. Making it much more easy to train. 
## 2015 IMAGENET (2015)
A bunch of classes, thousands. CVPR is one of the top conferences in Computer Vision. They have competitions to apply classes to these models. Watched a Ted Talk on this. 
## 2015 LSTM, RNN
Understand an image but also a video. Was able to learn and **generate** text. 
## 2017 Transformers
Attention is all you need. Instead of learning sequentially, you can also learn from different parts of the data. So it's important to also **not** pay attention to specific parts. 
#engs106 
